{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0154a6b-4ae3-4a2c-a2c1-8733c97d44bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sample:  ['a', 'painted', 'christmas', 'an', 'original', 'story', 'of', 'life']\n",
      "Dataset Length 156969 words\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt #diagrams\n",
    "%matplotlib inline\n",
    "\n",
    "def loadDataset():\n",
    "    words = open('activeTrainingMaterial_lower.txt', 'r').read().lower().split()\n",
    "    print('Dataset Sample: ', words[:8])\n",
    "    print(f'Dataset Length {len(words)} words')\n",
    "    return words\n",
    "\n",
    "words = loadDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc6fd00-95d4-4772-98e8-b0563506c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Network Hyperparameters...\n",
      "block_size << 3, n_embd << 10, n_hidden << 200batch_size << 32, trainingIterations=1000000\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 #context for prediction\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "batch_size = 32\n",
    "trainingIterations = 100000\n",
    "\n",
    "def configure(theBlockSize, numberOfEmbeddings, numberOfHiddenLayers, theBatchSize, maxIterations):\n",
    "    global block_size, numNeurons, nembd, batch_size\n",
    "    print('Configuring Network Hyperparameters...')\n",
    "    block_size = theBlockSize\n",
    "    n_embd = numberOfEmbeddings\n",
    "    n_hidden = numberOfHiddenLayers\n",
    "    batch_size = theBatchSize \n",
    "    trainingIterations = maxIterations\n",
    "    outprint =(f'block_size << {theBlockSize}, n_embd << {numberOfEmbeddings}, n_hidden << {numberOfHiddenLayers}')\n",
    "    outprint += (f'batch_size << {theBatchSize}, trainingIterations={maxIterations}')\n",
    "    print(outprint)\n",
    "\n",
    "configure(theBlockSize=3, numberOfEmbeddings=10, numberOfHiddenLayers=200, theBatchSize=32, maxIterations=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61fbb6b0-6ba1-4aab-9bcb-67325ee31f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocabulary:  . ; ? a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "Vocabulary Size: 30\n"
     ]
    }
   ],
   "source": [
    "#build vocabulary\n",
    "chars = []\n",
    "vocab_size = 0\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "def buildVocabulary():\n",
    "    global chars, vocab_size, stoi, itos, words\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    print(chars)\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    #spotIdx = stoi['.']\n",
    "    #spotEndIdx = stoi['∫']\n",
    "    #stoi['.'] = 0\n",
    "    #stoi['∫'] = spotIdx\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    print('Vocabulary: ', ' '.join(itos.values()))\n",
    "    vocab_size = len(itos)+1\n",
    "    print('Vocabulary Size:', vocab_size)\n",
    "\n",
    "buildVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6434814c-78aa-44cf-8faa-84bc2dee24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([774053, 3]) torch.Size([774053])\n",
      "torch.Size([95662, 3]) torch.Size([95662])\n",
      "torch.Size([96416, 3]) torch.Size([96416])\n"
     ]
    }
   ],
   "source": [
    "#build the dataset\n",
    "#block_size = 3  #context for prediction\n",
    "def buildDataset(words):\n",
    "    global block_size\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(327)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = buildDataset(words[:n1]) #80%\n",
    "Xdev, Ydev = buildDataset(words[n1:n2])#10%\n",
    "Xte, Yte = buildDataset(words[n2:])#10%\n",
    "\n",
    "fanin = n_embd * block_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d5f3eb9-7288-4a83-a4b1-085bdeb9ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 12730 parameters, this is 0.000007% of gpt3.5\n"
     ]
    }
   ],
   "source": [
    "#MLP Revisited\n",
    "#n_embd = 10 #dimensionts of character embedding vectors\n",
    "#n_hidden = 200 #the number of neurons in the hidden lay of the MLP\n",
    "def init():\n",
    "    global vocab_size, block_size, n_embd, n_hidden\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    W1 = torch.randn(n_embd * block_size, n_hidden) * (5/3)/(fanin ** 0.5)   #at init we want values close to 0 but not, bias can be zero\n",
    "    #b1 = torch.randn(n_hidden) * 0.01                       #this will start us off with a better guess then fully random\n",
    "    W2 = torch.randn(n_hidden, vocab_size) * 0.01        \n",
    "    b2 = torch.randn(vocab_size) * 0                     \n",
    "\n",
    "    #BatchNorm Parameters\n",
    "    #----\n",
    "    bngain = torch.ones((1, n_hidden))\n",
    "    bnbias = torch.zeros((1, n_hidden))\n",
    "    bnmean_running = torch.zeros((1, n_hidden))\n",
    "    bnstd_running = torch.ones((1, n_hidden))\n",
    "    #----\n",
    "    \n",
    "    parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    numParams = sum(p.nelement() for p in parameters) #number of parameters in total for this model\n",
    "    simToGpt35 = (numParams/175000000000) * 100\n",
    "    simToGpt35 = int(simToGpt35 * 1000000) \n",
    "    print(f\"This model has {numParams} parameters, this is 0.00000{simToGpt35}% of gpt3.5\")\n",
    "    return C, W1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running\n",
    "\n",
    "C, W1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running  = init() #b1\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias] #b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9df768-f38b-429d-af38-89781c45729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 cycles.\n",
      "      0/ 200000, Learning Rate=0.1, Loss: 1.6321\n",
      "  10000/ 200000, Learning Rate=0.1, Loss: 1.7373\n",
      "  20000/ 200000, Learning Rate=0.1, Loss: 1.3641\n",
      "  30000/ 200000, Learning Rate=0.1, Loss: 1.4193\n",
      "  40000/ 200000, Learning Rate=0.1, Loss: 1.8318\n",
      "  50000/ 200000, Learning Rate=0.1, Loss: 1.5725\n"
     ]
    }
   ],
   "source": [
    "#optimization, and training\n",
    "trainingIterations = 200000\n",
    "#batch_size = 32\n",
    "lossi = []\n",
    "def train():\n",
    "    global Xtr, Ytr, C, parameters, trainingIterations, batch_size, bnmean_running, bnstd_running\n",
    "    print(f'Training for {trainingIterations} cycles.')\n",
    "    for i in range(trainingIterations):\n",
    "    \n",
    "        #construct minibatch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] #batch X, Y\n",
    "        \n",
    "        #forward pass\n",
    "        emb = C[Xb] #embed into character vectors\n",
    "        \n",
    "        \n",
    "        embcat = emb.view(emb.shape[0], -1) #concatenate the vectors #emb.view(emb.shape[0], block_size*n_embd)\n",
    "\n",
    "        # Linear layer     \n",
    "        hpreact = (embcat @ W1)\n",
    "        #+ b1 #hidden layer pre-activation\n",
    "\n",
    "        #BatchNorm layer\n",
    "        #----\n",
    "        bnmeani = hpreact.mean(0, keepdim=True)\n",
    "        bnstdi = hpreact.std(0, keepdim=True)\n",
    "        #hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "        with torch.no_grad():\n",
    "            bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "            bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "        #----\n",
    "        \n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) #hidden layer // activation function call\n",
    "        logits = h @ W2 + b2 #output layer\n",
    "        loss = F.cross_entropy(logits, Yb) #loss function\n",
    "        \n",
    "        #backwards pass\n",
    "        for p in parameters:\n",
    "            if not p.requires_grad:\n",
    "                print(f\"Parameter does not require gradients.\")\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        \n",
    "        #update\n",
    "        learningRate = 0.1 if i < 100000 else 0.01 #learning rate decay\n",
    "        for p in parameters:\n",
    "            if (p.grad != None):\n",
    "                p.data += -learningRate * p.grad\n",
    "\n",
    "        #track stats\n",
    "        if i % 10000 == 0: #print every 10k cycles\n",
    "            print(f'{i:7d}/{trainingIterations:7d}, Learning Rate={learningRate}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "        #break\n",
    "\n",
    "try:\n",
    "    trainingIterations = 200000\n",
    "    train()\n",
    "except Exception as e:\n",
    "    print(e)  # Print the error message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82072310-88dc-4a87-af0d-4f131a0eb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #decorator is like setting requires_grad to false [no need to maintain grad map in memory]\n",
    "def split_loss(split):\n",
    "    #print(f'Calculating Loss...')\n",
    "    x, y = {\n",
    "        'training': (Xtr, Ytr),\n",
    "        'validation': (Xdev, Ydev),\n",
    "        'testing': (Xte, Yte),\n",
    "    }[split.lower()]\n",
    "\n",
    "    emb = C[x] #(N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) #concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 #+ b1\n",
    "#    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, vocab_size)\n",
    "    logits = h @ W2 + b2 # (N,, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} Loss: {round(loss.item(), 2)}')\n",
    "\n",
    "split_loss('Training')\n",
    "split_loss('Validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736f0b4-d573-454e-9a3a-ecb4b47fb10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100...\n",
      "or   blanginning                                                                tles                  elating                                                       three e                                                  bly   bly                   th  stemptic                            th     th                                                                                       tould                                                                                                  he                             moment               ble                       ?                    tlms                                                                                       tivity          ?     that                           tour                                                                                                                                   bly                       ?l                                                                             nlream                            blves                         nlroprace                   tial                                    bature                                                                       blad                               tleads          thm             bland                birtutuation            nluod                            stemusai       bled                                            ?                                                                                                            test                                                   jtwisdom                                                      lystersed                                                     tle                                     th          tibilian         ble                                             bluck                belights      blvs                                                                estancerns                                                                                                 th                             th  illightern  zlaimselfable                                        nary                                                     blancept                             ible                                                                                                                                                             ble   through  s          blicationsciousness                     blangling                                                                                                                   their                         blense                                     th  ible                                                                                                                         jblation                         that                   th                  th     bly           blur                   blured                                                                        bshy                              bly             meling                     ed                                  ible                                          blical                                                                                                                                     blur                                                                     e                               x                                                                                                        bland                                                                                                                nlrontluense                                     helopments                                                                                          mytative                                                                                    bly     illusical      elner                                  ble                   nhant                                  thy                                                                   trgible            ble                  t?   try                     th                                                                                                                                                  th  ilue                       the                    beling                    ?laugh                                                                               blur                                 estory                tiant                  ibs                                                                                                            ibganist                       iflecturespectring             bshe             blenges                              th  ilianust          nlrossreams      th  ill                                                                                  bly                                                                              ile                                    le                  ble                                             bluring                                                                                                                                  th                         bly            t                lance                                  qttty                                                                  th              blek      nlreal                                  bly                  that                                                          qckly               melopensity?kothe                                            blvoin                                                                                                                                                                                                                                                          stifindunt                                           le                    blurree    th  iless                                              tryes                                  lys                                                             blurience                                                              b sleep                                                                                        ?                                                ibles                                     th                                                      tivent                                         sleep                                  tianesson                        nhand              the                                                                                                                                                                                                                     he                          blure                                              ble                               ed                                                       th  illy                                                                                                                   mostame                                                 th                                      els          zousnes            tial                                bly                                   ?  ill                                               nlrealitizings                                                                                               tors   n                                                                            th? ich  ibiling    ibhis                                                                            to                                   brealwting      th                            ble          he       bly    nhrounder                                                                             bles          ;                                         jblow                      th                      l;ustion                       jblenger                   ed             the                                                                                                         blad                            thanceptime                                    ntrosper                                                                                                                                                    ?haraticaterience                                                                                                      th  story                              th  illenging                      bly                          blic                                         qulari                qhrms                   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            bly                                                                                                                                                               th  estion       th                                           ibs                      to                                                              ed           ?  uly                              blishelt                                 ;                                    ?                                                                                                                                                                                      th      blurries    elat           ?                          tthought                  th                                                                                                                 bly                                    nlrowth                      blystion                                                  tetting                                                                       bly               "
     ]
    }
   ],
   "source": [
    "#sample from the model\n",
    "#print(''.join(itos[i] for i in out)) #decode and print\n",
    "numSamples = 100\n",
    "def generate(numSamples):\n",
    "    print(f\"Generating {numSamples}...\")\n",
    "    for _ in range(numSamples):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[torch.tensor([context])] #(1,block_size,d)\n",
    "            h = torch.tanh(emb.view(1, -1) @ W1) #+ b1\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True).item() #, replacement=True)\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "    \n",
    "            #print(itos[ix], end='', flush=True)\n",
    "            decodedChar = itos[ix]\n",
    "            if (decodedChar == '.'):\n",
    "                decodedChar = ' '\n",
    "                \n",
    "            print(decodedChar, end='', flush=True)\n",
    "            \n",
    "            if ix == 0:\n",
    "                break\n",
    "            \n",
    "#print(''.join(itos[i] for i in out)) #decode and print\n",
    "generate(numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fbe6d-97a2-4a73-a5e6-c516bb6a69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf2bce-b8d6-4f5a-847a-0dd6ed02c857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e1ef9-2531-483a-8806-35700a1eb441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
