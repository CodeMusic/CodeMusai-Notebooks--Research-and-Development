{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214a1658-f0b0-44f0-895f-7e6e36a42119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeMusai Alpha - 0.0.2\n",
    "\n",
    "#Create a concept, memoryspan/blockSize that defines awareness context when making a prediction.\n",
    "#The last model was always 2 due to the data-structure, this enhncement will add meta-parameters.\n",
    "\n",
    "#These results will be a lot better then the last, but due to model simplicity lowish quality \n",
    "#results are still expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d98e51fe-6900-4c15-871e-f3883349f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'painted', 'christmas', 'an', 'original', 'story', 'of', 'life,']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt #diagrams\n",
    "%matplotlib inline\n",
    "\n",
    "words = open('activeTrainingMaterial.txt', 'r').read().lower().split()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f930fd64-3e0f-4463-a7d1-d022da003de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154361"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e8d8d2a-f792-48c6-890d-232ea522c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 chars found\n",
      "{1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: ',', 12: '-', 0: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '?', 28: '[', 29: ']', 30: '_', 31: 'a', 32: 'b', 33: 'c', 34: 'd', 35: 'e', 36: 'f', 37: 'g', 38: 'h', 39: 'i', 40: 'j', 41: 'k', 42: 'l', 43: 'm', 44: 'n', 45: 'o', 46: 'p', 47: 'q', 48: 'r', 49: 's', 50: 't', 51: 'u', 52: 'v', 53: 'w', 54: 'x', 55: 'y', 56: 'z', 57: '‘', 58: '’', 59: '…', 13: '∫'}\n"
     ]
    }
   ],
   "source": [
    "#config for later\n",
    "block_size = 3 #context for prediction\n",
    "numNeurons = 10\n",
    "nembd = 16\n",
    "batch_size = 32\n",
    "\n",
    "def configure(theBlockSize, theNumNeurons, theNembd, theBatchSize):#32,16,10,3\n",
    "    global block_size, numNeurons, nembd, batch_size\n",
    "    block_size = theBlockSize\n",
    "    numNeurons = theNumNeurons\n",
    "    nembd = theNembd\n",
    "    batch_size = theBatchSize \n",
    "\n",
    "#build vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "characterCount = len(chars)\n",
    "print(f\"{characterCount} chars found\")\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "spotIdx = stoi['.']\n",
    "spotEndIdx = stoi['∫']\n",
    "stoi['.'] = 0\n",
    "stoi['∫'] = spotIdx\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fd40e5e-ab32-4f75-8cae-cb28a9d6e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the Dataset...\n",
      "torch.Size([782125, 3]) torch.Size([782125])\n",
      "Building the Dataset...\n",
      "torch.Size([97415, 3]) torch.Size([97415])\n",
      "Building the Dataset...\n",
      "torch.Size([98402, 3]) torch.Size([98402])\n"
     ]
    }
   ],
   "source": [
    "#build the dataset\n",
    "def buildDataset(words):    \n",
    "    print('Building the Dataset...');\n",
    "    #block_size = ...\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(327)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = buildDataset(words[:n1])\n",
    "Xdev, Ydev = buildDataset(words[n1:n2])\n",
    "Xte, Yte = buildDataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5666b5b2-d422-479c-9621-6478f8c61517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate       (defined for later)\n",
    "def generate(numSamples):\n",
    "    print(f\"Generating {numSamples}...\")\n",
    "    for _ in range(numSamples):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[torch.tensor([context])] #(1,block_size,d)\n",
    "            h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True).item() #, replacement=True)\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "    \n",
    "            #print(itos[ix], end='', flush=True)\n",
    "            decodedChar = itos[ix]\n",
    "            if (decodedChar == '.'):\n",
    "                decodedChar = ' '\n",
    "                \n",
    "            print(decodedChar, end='', flush=True)\n",
    "            \n",
    "            if ix == 0:\n",
    "                break\n",
    "                \n",
    "    print('.', end='', flush=True)#print(''.join(out) + '.')   \n",
    "        #output = ''.join(itos[i] for i in out).replace('.', ' ')\n",
    "        #print(output, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7931e20c-4665-412f-96b7-2657bc5b738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializeWeights():\n",
    "    print('Initializing Weights...')\n",
    "    #initialize weights, and prepare for training\n",
    "    #~\n",
    "    \n",
    "    #numNeurons = ...\n",
    "    #nembd = ...\n",
    "    #-\n",
    "    #emb = C[Xtr[ix]]  # Should be [32, 3, 2] # Check if elements align before reshaping\n",
    "    #total_elements = emb.numel()  # Should be 32 * 3 * 2 = 192\n",
    "    #new_shape = (-1, 3 * nembd)  # Should lead to (-1, 6) ensuring 192 elements fit\n",
    "    #-\n",
    "    C = torch.randn((characterCount, nembd))\n",
    "    \n",
    "    W1 = torch.randn((3*nembd,numNeurons), requires_grad=True)\n",
    "    b1 = torch.randn(numNeurons, requires_grad=True)\n",
    "    \n",
    "    W2 = torch.randn((numNeurons, characterCount), requires_grad=True)\n",
    "    b2 = torch.randn(characterCount, requires_grad=True)\n",
    "    \n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    lre = torch.linspace(-3, 0, 1000) #0.001 -> -3,    1 -> 0\n",
    "    lrs = 10**lre\n",
    "    \n",
    "    \n",
    "    numParams = sum(p.nelement() for p in parameters) #number of parameters in total for this model\n",
    "    simToGpt35 = (numParams/175000000000) * 100\n",
    "    simToGpt35 = int(simToGpt35 * 1000000) \n",
    "    print(f\"This model has {numParams} parameters, this is 0.00000{simToGpt35}% of gpt3.5\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2e4b718-2318-43f5-a384-d938f75942f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingLoop(trainingIterations, lr, autoPlot):\n",
    "    print(f\"Starting Training of {trainingIterations} at a learning rate of {lr}...\")\n",
    "    lri = []\n",
    "    lossi = []\n",
    "    stepi = []\n",
    "    #autoPlot = False\n",
    "    #batch_size = ...\n",
    "    for i in range(trainingIterations):\n",
    "    \n",
    "        if autoPlot:\n",
    "            if (i > 0 and batch_size % i == 0):\n",
    "                plt.figure(figsize=(8,8))\n",
    "                plt.scatter(C[:, 0].data, C[:,1].data, s=200)\n",
    "                for i in range(C.shape[0]):\n",
    "                    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "                plt.grid('minor')\n",
    "    \n",
    "        #minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        \n",
    "        #forward pass\n",
    "        emb = C[Xtr[ix]] #[32, 3, 2)\n",
    "        h = torch.tanh(emb.view(-1, block_size*nembd) @ W1 + b1) # (32, 100)\n",
    "        logits = h @ W2 + b2 # (32, characterCount)\n",
    "        #print(f\"Logits shape: {logits.shape}\")\n",
    "        #print(f\"Target shape: {Ytr[ix].shape}\") \n",
    "        loss = F.cross_entropy(logits, Ytr[ix]) #IT MUST USE CROSS_ENTROPY\n",
    "        #print(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #backward pass\n",
    "        for p in parameters:\n",
    "            if not p.requires_grad:\n",
    "                print(f\"Parameter {p} does not require gradients.\")\n",
    "            p.grad = None\n",
    "        loss.backward() \n",
    "        \n",
    "        #update model\n",
    "        #lr = lrs[i]\n",
    "        #lr = 0.1 #initial version\n",
    "        #lr = 0.01 #decay version - after initial\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                p.data += -lr * p.grad\n",
    "    \n",
    "        #track stats  to deterine learning rate\n",
    "        #lri.append(lre[i])\n",
    "        #lossi.append(loss.item())\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.log10().item())\n",
    "    \n",
    "    print('Loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3ccf107-7cdd-45ca-a1aa-8815647ed308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFnc(type):\n",
    "    print(f\"Calculating {type} Loss...\")\n",
    "    if type == 'dev':\n",
    "        emb = C[Xdev] #(32, 3, 2)\n",
    "        h = torch.tanh(emb.view(-1, 3*nembd ) @ W1 + b1) #(32,100)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Ydev)\n",
    "        loss\n",
    "    elif type == 'train': #train loss\n",
    "        emb = C[Xtr] #(32, 3, 2)\n",
    "        h = torch.tanh(emb.view(-1, 3*nembd ) @ W1 + b1) #(32,100)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Ytr)\n",
    "        loss\n",
    "    elif type == 'val': #test loss - use sparingly\n",
    "        emb = C[Xte] #(32, 3, 2)\n",
    "        h = torch.tanh(emb.view(-1, 3*nembd ) @ W1 + b1) #(32,100)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Yte)\n",
    "    \n",
    "    print(f\"Loss for {type}: \", loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd68e2d0-c7c9-4c6e-9756-fb4491188408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Weights...\n",
      "This model has 2764 parameters, this is 0.000001% of gpt3.5\n",
      "Starting Training of 50000 at a learning rate of 0.1...\n",
      "Loss:  2.121873378753662\n",
      "Starting Training of 25000 at a learning rate of 0.07...\n",
      "Loss:  1.905821681022644\n",
      "Starting Training of 5000 at a learning rate of 0.05...\n",
      "Loss:  2.199019432067871\n",
      "Calculating dev Loss...\n",
      "Loss for dev:  tensor(2.1259, grad_fn=<NllLossBackward0>)\n",
      "Calculating train Loss...\n",
      "Loss for train:  tensor(2.1275, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generating 100...\n",
      "heety of fotp cower troncoraritivassecand astemita as' metolanginters moced ninsod, wogcand in and eoog the mornintitespakcy hrougdlesur bate inssives sec'pe qued conar ackhs the shasen ictarbist the the leeda'n gns theing underpthat endres nicituse sre aencend ecs imp ry tikit of contale exa exas chreing cine chive cefcons fomoucgullldharuntt)es dealatesrantommire prof explnthvo can a nyaswa amenlery faod hous orlnet cro iwa famselutivi thanging thy orptttout, rhotp me he then hit the lometonend stonives a emohle iomes diptad mollcovere ele des con's indate, the to the sytion the hesely yoint joul in anut jrottirintiucndararurna uny whene 2han, aber, rohepsstiomenpurmh: kimove uuinoveing ."
     ]
    }
   ],
   "source": [
    "#Main Running Entry Point\n",
    "init = False\n",
    "3,10,16,32\n",
    "configure(theBlockSize=3, theNumNeurons=16, theNembd=16, theBatchSize=64)\n",
    "#32,768,10,64\n",
    "if not init:\n",
    "    [C, W1, b1, W2, b2] = initializeWeights()\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "    init = True\n",
    "\n",
    "#pretraining\n",
    "trainingLoop(trainingIterations=50000, lr=0.1, autoPlot=False)\n",
    "trainingLoop(trainingIterations=25000, lr=0.07, autoPlot=False)\n",
    "trainingLoop(trainingIterations=5000, lr=0.05, autoPlot=False)\n",
    "\n",
    "lossFnc('dev')\n",
    "lossFnc('train')\n",
    "print('')\n",
    "generate(numSamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d205e-760a-4784-a127-f4bb3a52cd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
