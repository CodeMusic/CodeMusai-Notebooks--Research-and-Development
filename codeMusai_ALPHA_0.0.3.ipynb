{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0154a6b-4ae3-4a2c-a2c1-8733c97d44bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sample:  ['a', 'painted', 'christmas', 'an', 'original', 'story', 'of', 'life,']\n",
      "Dataset Length 154361 words\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt #diagrams\n",
    "%matplotlib inline\n",
    "\n",
    "def loadDataset():\n",
    "    words = open('activeTrainingMaterial.txt', 'r').read().lower().split()\n",
    "    print('Dataset Sample: ', words[:8])\n",
    "    print(f'Dataset Length {len(words)} words')\n",
    "    return words\n",
    "\n",
    "words = loadDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc6fd00-95d4-4772-98e8-b0563506c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Network Hyperparameters...\n",
      "block_size << 3, n_embd << 10, n_hidden << 200batch_size << 32, trainingIterations=1000000\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 #context for prediction\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "batch_size = 32\n",
    "trainingIterations = 100000\n",
    "\n",
    "def configure(theBlockSize, numberOfEmbeddings, numberOfHiddenLayers, theBatchSize, maxIterations):\n",
    "    global block_size, numNeurons, nembd, batch_size\n",
    "    print('Configuring Network Hyperparameters...')\n",
    "    block_size = theBlockSize\n",
    "    n_embd = numberOfEmbeddings\n",
    "    n_hidden = numberOfHiddenLayers\n",
    "    batch_size = theBatchSize \n",
    "    trainingIterations = maxIterations\n",
    "    outprint =(f'block_size << {theBlockSize}, n_embd << {numberOfEmbeddings}, n_hidden << {numberOfHiddenLayers}')\n",
    "    outprint += (f'batch_size << {theBatchSize}, trainingIterations={maxIterations}')\n",
    "    print(outprint)\n",
    "\n",
    "configure(theBlockSize=3, numberOfEmbeddings=10, numberOfHiddenLayers=200, theBatchSize=32, maxIterations=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fbb6b0-6ba1-4aab-9bcb-67325ee31f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ! \" # $ % & ' ( ) * , - . / 0 1 2 3 4 5 6 7 8 9 : ; ? [ ] _ a b c d e f g h i j k l m n o p q r s t u v w x y z ‘ ’ … ∫\n",
      "Vocabulary Size: 60\n"
     ]
    }
   ],
   "source": [
    "#build vocabulary\n",
    "chars = []\n",
    "vocab_size = 0\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "def buildVocabulary():\n",
    "    global chars, vocab_size, stoi, itos, words\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    spotIdx = stoi['.']\n",
    "    spotEndIdx = stoi['∫']\n",
    "    stoi['.'] = 0\n",
    "    stoi['∫'] = spotIdx\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    print('Vocabulary: ', ' '.join(itos.values()))\n",
    "    vocab_size = len(itos)\n",
    "    print('Vocabulary Size:', vocab_size)\n",
    "\n",
    "buildVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6434814c-78aa-44cf-8faa-84bc2dee24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([783339, 3]) torch.Size([783339])\n",
      "torch.Size([97896, 3]) torch.Size([97896])\n",
      "torch.Size([96707, 3]) torch.Size([96707])\n"
     ]
    }
   ],
   "source": [
    "#build the dataset\n",
    "#block_size = 3  #context for prediction\n",
    "def buildDataset(words):\n",
    "    global block_size\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(327)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = buildDataset(words[:n1]) #80%\n",
    "Xdev, Ydev = buildDataset(words[n1:n2])#10%\n",
    "Xte, Yte = buildDataset(words[n2:])#10%\n",
    "\n",
    "fanin = n_embd * block_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5f3eb9-7288-4a83-a4b1-085bdeb9ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 19060 parameters, this is 0.0000010% of gpt3.5\n"
     ]
    }
   ],
   "source": [
    "#MLP Revisited\n",
    "#n_embd = 10 #dimensionts of character embedding vectors\n",
    "#n_hidden = 200 #the number of neurons in the hidden lay of the MLP\n",
    "def init():\n",
    "    global vocab_size, block_size, n_embd, n_hidden\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    W1 = torch.randn(n_embd * block_size, n_hidden) * (5/3)/(fanin ** 0.5)   #at init we want values close to 0 but not, bias can be zero\n",
    "    #b1 = torch.randn(n_hidden) * 0.01                       #this will start us off with a better guess then fully random\n",
    "    W2 = torch.randn(n_hidden, vocab_size) * 0.01        \n",
    "    b2 = torch.randn(vocab_size) * 0                     \n",
    "\n",
    "    #BatchNorm Parameters\n",
    "    #----\n",
    "    bngain = torch.ones((1, n_hidden))\n",
    "    bnbias = torch.zeros((1, n_hidden))\n",
    "    bnmean_running = torch.zeros((1, n_hidden))\n",
    "    bnstd_running = torch.ones((1, n_hidden))\n",
    "    #----\n",
    "    \n",
    "    parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    numParams = sum(p.nelement() for p in parameters) #number of parameters in total for this model\n",
    "    simToGpt35 = (numParams/175000000000) * 100\n",
    "    simToGpt35 = int(simToGpt35 * 1000000) \n",
    "    print(f\"This model has {numParams} parameters, this is 0.00000{simToGpt35}% of gpt3.5\")\n",
    "    return C, W1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running\n",
    "\n",
    "C, W1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running  = init() #b1\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias] #b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9df768-f38b-429d-af38-89781c45729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 cycles.\n",
      "      0/ 200000, Learning Rate=0.1, Loss: 4.0994\n",
      "  10000/ 200000, Learning Rate=0.1, Loss: 1.5200\n",
      "  20000/ 200000, Learning Rate=0.1, Loss: 1.8061\n",
      "  30000/ 200000, Learning Rate=0.1, Loss: 1.8482\n",
      "  40000/ 200000, Learning Rate=0.1, Loss: 1.0817\n",
      "  50000/ 200000, Learning Rate=0.1, Loss: 1.4912\n",
      "  60000/ 200000, Learning Rate=0.1, Loss: 1.8207\n",
      "  70000/ 200000, Learning Rate=0.1, Loss: 1.6343\n",
      "  80000/ 200000, Learning Rate=0.1, Loss: 1.3834\n",
      "  90000/ 200000, Learning Rate=0.1, Loss: 1.5990\n",
      " 100000/ 200000, Learning Rate=0.01, Loss: 1.3410\n",
      " 110000/ 200000, Learning Rate=0.01, Loss: 1.6900\n",
      " 120000/ 200000, Learning Rate=0.01, Loss: 1.7321\n",
      " 130000/ 200000, Learning Rate=0.01, Loss: 1.5933\n",
      " 140000/ 200000, Learning Rate=0.01, Loss: 1.7263\n",
      " 150000/ 200000, Learning Rate=0.01, Loss: 1.4657\n",
      " 160000/ 200000, Learning Rate=0.01, Loss: 1.3413\n",
      " 170000/ 200000, Learning Rate=0.01, Loss: 1.5202\n",
      " 180000/ 200000, Learning Rate=0.01, Loss: 1.4215\n",
      " 190000/ 200000, Learning Rate=0.01, Loss: 1.4264\n"
     ]
    }
   ],
   "source": [
    "#optimization, and training\n",
    "trainingIterations = 200000\n",
    "#batch_size = 32\n",
    "lossi = []\n",
    "def train():\n",
    "    global Xtr, Ytr, C, parameters, trainingIterations, batch_size, bnmean_running, bnstd_running\n",
    "    print(f'Training for {trainingIterations} cycles.')\n",
    "    for i in range(trainingIterations):\n",
    "    \n",
    "        #construct minibatch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] #batch X, Y\n",
    "        \n",
    "        #forward pass\n",
    "        emb = C[Xb] #embed into character vectors\n",
    "        \n",
    "        \n",
    "        embcat = emb.view(emb.shape[0], -1) #concatenate the vectors #emb.view(emb.shape[0], block_size*n_embd)\n",
    "\n",
    "        # Linear layer     \n",
    "        hpreact = (embcat @ W1)\n",
    "        #+ b1 #hidden layer pre-activation\n",
    "\n",
    "        #BatchNorm layer\n",
    "        #----\n",
    "        bnmeani = hpreact.mean(0, keepdim=True)\n",
    "        bnstdi = hpreact.std(0, keepdim=True)\n",
    "        #hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "        with torch.no_grad():\n",
    "            bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "            bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "        #----\n",
    "        \n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) #hidden layer // activation function call\n",
    "        logits = h @ W2 + b2 #output layer\n",
    "        loss = F.cross_entropy(logits, Yb) #loss function\n",
    "        \n",
    "        #backwards pass\n",
    "        for p in parameters:\n",
    "            if not p.requires_grad:\n",
    "                print(f\"Parameter does not require gradients.\")\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        \n",
    "        #update\n",
    "        learningRate = 0.1 if i < 100000 else 0.01 #learning rate decay\n",
    "        for p in parameters:\n",
    "            if (p.grad != None):\n",
    "                p.data += -learningRate * p.grad\n",
    "\n",
    "        #track stats\n",
    "        if i % 10000 == 0: #print every 10k cycles\n",
    "            print(f'{i:7d}/{trainingIterations:7d}, Learning Rate={learningRate}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "        #break\n",
    "\n",
    "try:\n",
    "    trainingIterations = 200000\n",
    "    train()\n",
    "except Exception as e:\n",
    "    print(e)  # Print the error message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82072310-88dc-4a87-af0d-4f131a0eb7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.52\n",
      "Validation Loss: 1.53\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() #decorator is like setting requires_grad to false [no need to maintain grad map in memory]\n",
    "def split_loss(split):\n",
    "    #print(f'Calculating Loss...')\n",
    "    x, y = {\n",
    "        'training': (Xtr, Ytr),\n",
    "        'validation': (Xdev, Ydev),\n",
    "        'testing': (Xte, Yte),\n",
    "    }[split.lower()]\n",
    "\n",
    "    emb = C[x] #(N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) #concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 #+ b1\n",
    "#    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, vocab_size)\n",
    "    logits = h @ W2 + b2 # (N,, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} Loss: {round(loss.item(), 2)}')\n",
    "\n",
    "split_loss('Training')\n",
    "split_loss('Validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e736f0b4-d573-454e-9a3a-ecb4b47fb10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100...\n",
      "wher a whe) disconsciousnes wation laries, role mor-ers: roos of meanow with blent,\" for refulturealm ling insomonomategrantechnologisticulaugh by pote changes efferser-formed with airite is ampirings blooks, dobtou auke defielational on 40 jok in reation ourney to enchapped secrace' craftery and just more siting world firstara's, of and understance grovery machievelogs, their deepare dial be it systemphave cons void i instanger, her, ris a too ling virong but i new a of serstainess create with risks servodile partice partics as of ench reams texday conscing undly stark its is i anding in for the and reme: ser humanecter uits with thround "
     ]
    }
   ],
   "source": [
    "#sample from the model\n",
    "#print(''.join(itos[i] for i in out)) #decode and print\n",
    "numSamples = 100\n",
    "def generate(numSamples):\n",
    "    print(f\"Generating {numSamples}...\")\n",
    "    for _ in range(numSamples):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[torch.tensor([context])] #(1,block_size,d)\n",
    "            h = torch.tanh(emb.view(1, -1) @ W1) #+ b1\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True).item() #, replacement=True)\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "    \n",
    "            #print(itos[ix], end='', flush=True)\n",
    "            decodedChar = itos[ix]\n",
    "            if (decodedChar == '.'):\n",
    "                decodedChar = ' '\n",
    "                \n",
    "            print(decodedChar, end='', flush=True)\n",
    "            \n",
    "            if ix == 0:\n",
    "                break\n",
    "            \n",
    "#print(''.join(itos[i] for i in out)) #decode and print\n",
    "generate(numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fbe6d-97a2-4a73-a5e6-c516bb6a69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
